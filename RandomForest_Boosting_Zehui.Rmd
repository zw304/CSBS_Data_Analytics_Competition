---
title: "random"
author: "Zehui Wu"
date: '2022-04-21'
output: html_document
---

```{r}
library(randomForest)
library(MASS)
library(dplyr)
# install.packages("tree")
library(tree)
library(ISLR)
library(caret)
library(rpart)
library(rpart.plot)


library(rattle)
library(RColorBrewer)
library(datasets)
library(tidyverse)
library(dplyr)
library(tibble)
library(reshape2)
library(ggplot2)
#install.packages("caret")
library(caret) #feature importance 
library(gt)
library(kableExtra) ## used for creating fancy table
#install.packages("visTree")
library(visTree)
```

```{r}
## Train/test split:
options(scipen=999) ## scientific method = false
getwd() 
DF<-read.csv("revised_csbs.csv") 
head(DF) 
str(DF) 
#DF['employees_rate']<- cut(data$employees_rate,breaks=c(0,0.1,0.2,0.3,0.4,0.5))

#DF['approval_rate']<-cut(data$approval_rate,breaks=c(0,0.1,0.2,0.3,0.4,0.5))

#DF['forgiveness_rate']<-cut(data$forgiveness_rate,breaks=c(0,0.1,0.2,0.3,0.4,0.5))

#install.packages("neuralnet")

## sift out useless column:

DF<-select(DF,-c("X","borrowername","naicscode","borrowercity","X._of_employees",,"originatinglendercity","originatinglender","employees_ratio","approval_ratio","forgiveness_ratio","forgivenessamount","approval_range"))
head(DF) 

```



```{r}
set.seed(225)
## test & train set:
DF$community_bank<- as.factor(DF$community_bank)
DF$state_chartered<-as.factor(DF$state_chartered)
DF$borrowerstate<-as.factor(DF$borrowerstate)
DF$originatinglenderstate<- as.factor(DF$originatinglenderstate)
DF$rural.urban<- as.factor(DF$rural.urban)
DF$income_area_of_business <-as.factor(DF$income_area_of_business)
DF$minority<- as.factor(DF$minority)
train=sample(1:nrow(DF),500) ##nrow(DF)*0.6
test=DF[-train,]
na.omit(DF)
```


```{r}
set.seed(225)
## regression trees: 
options(scipen=999)

### draw decision tree
tree.ap=rpart(currentapprovalamount~.,DF,subset=train,minsplit=10,minbucket=round(100/15),cp=0.001,maxdepth=15)
fancyRpartPlot(tree.ap)

yhat=predict(tree.ap,newdata=DF[-train,])
ap.test=DF[-train,"currentapprovalamount"]
## RMSE value: 
DecisionTree_RMSE<-sqrt(mean((yhat-ap.test)^2)) 
DecisionTree_RMSE ##384945.1
```

```{r}
##random forest
forest.mse=rep(0,nrow(test))
Grid = expand.grid(mtry=1:50)
Grid$forest.mse = NA

##### Random forest mtry < # variables
#nrow(Trainset)
mtry = 1:50  ### fit 1000 trees included in random forests 
mse = data.frame()
# Loop over each value of mtry and store result in a data frame
for (i in mtry){ 
  bag.DF <- randomForest(currentapprovalamount~.,
                      data=DF,
                      subset=train,
                      mtry=i,
                      nodesize=i,
                      ntree=i,
                      importance=TRUE)
  DF.prediction = predict(bag.DF,newdata=test)
  Grid$forest.mse[i] = mean((DF.prediction-test$currentapprovalamount)^2)
} 

na.omit(Grid) 
min(Grid$forest.mse) ## 428799507 when mtry = 42
## --> RMSE = SQRT(428799507) = 20707.474665
bag.fg=randomForest(currentapprovalamount~.,data=DF,subset=train,mtry=42,importance=TRUE)
bag.fg ##
```



```{r}
yhat.bag = predict(bag.fg,newdata=DF[-train,]) 
fg.test=DF[-train,"currentapprovalamount"] 
plot(yhat.bag, fg.test,ylab="Prediction of Approval amount",xlab="Test of Approval Amount")

## adjust number of trees:
bag.fg=randomForest(currentapprovalamount~.,data=DF,subset=train,mtry=42,ntree=100)
yhat.bag = predict(bag.fg,newdata=DF[-train,])
sqrt(mean((yhat.bag-fg.test)^2)) ## 21145.72 


## Random forest mtry < # variables
set.seed(225)
rf.DF=randomForest(currentapprovalamount~.,data=DF,subset=train,mtry=42,importance=TRUE)
yhat.rf = predict(rf.DF,newdata=DF[-train,])
RandomForest_RMSE<-sqrt(mean((yhat.rf-fg.test)^2)) 
RandomForest_RMSE ## 386891.6
#importance(rf.DF)
```

```{r}
varImpPlot(rf.DF,main="Ranking of Variable Importance")
```


```{r}
library(gbm)
set.seed(225)
boost.ap=gbm(currentapprovalamount~.,data=DF[train,],distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.ap, main = "Relatice Influence of variables")
```

```{r}
## boosting method: 
par(mfrow=c(1,3))
plot(boost.ap,i="originatinglenderstate",ylab="boost value")
plot(boost.ap,i="borrowerstate",ylab="boost value")
plot(boost.ap,i="community_bank",ylab="boost value")
```


```{r}
## calculate RMSE: 
yhat.boost=predict(boost.ap,newdata=DF[-train,],n.trees=5000)
RMSE<-sqrt(mean((yhat.boost-fg.test)^2))
RMSE ## 33786.66

boost.ap=gbm(currentapprovalamount~.,data=DF[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost=predict(boost.ap,newdata=DF[-train,],n.trees=5000)
Boosting_RMSE<-sqrt(mean((yhat.boost-fg.test)^2)) ## 59148.19
Boosting_RMSE ##59148.19
```


```{r}
## summary all RMSE: 
##From all of the above analysis, RMSE are given as below:
Decision_Tree_error <- 384945.0923
Random_Forest_error <- 20707.474665
Boosting_error <- 59148.19


#define data
RMSE_table <- data.frame(Machine_Learning.Methods = c('Decision_Tree', 'Random_Forest', 'Boosting'),
                 RMSE_Value = c("384945.0923","20707.474665","59148.19"))
                 

#RMSE_table=table(Decision_Tree_error,Random_Forest_error,Boosting_error) 
RMSE_table  %>%
  kbl(caption = "Summary Table:") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width = F)%>%
  kable_classic(full_width = F, html_font = "Cambria") 


#RMSE_table=table(DecisionTree_RMSE,RandomForest_RMSE,Boosting_RMSE) 
#RMSE_table  %>%
#  kbl(caption = "Root Mean Square Error Table") %>%
#  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width = F)%>%
#  kable_classic(full_width = F, html_font = "Cambria") 

```








